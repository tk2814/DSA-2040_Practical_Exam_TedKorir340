{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bc722b",
   "metadata": {},
   "source": [
    "# Task 3 â€“ Classification and Association Rule Mining\n",
    "\n",
    "## Part A: Classification\n",
    "\n",
    "The **Decision Tree classifier** achieved higher accuracy than **KNN (k=5)** in this experiment.  \n",
    "Metrics for Decision Tree were:\n",
    "\n",
    "- Accuracy: **0.97**\n",
    "- Precision: **0.97**\n",
    "- Recall: **0.97**\n",
    "- F1-score: **0.97**\n",
    "\n",
    "The KNN model scored slightly lower due to sensitivity to feature scaling and nearest neighbor distances.  \n",
    "The Decision Tree was also easier to interpret, as shown in the tree visualization.\n",
    "\n",
    "## Part B: Association Rule Mining\n",
    "\n",
    "Using synthetic transactional data of 30 baskets, the **Apriori algorithm** was applied with:\n",
    "\n",
    "- min_support = 0.2\n",
    "- min_confidence = 0.5\n",
    "\n",
    "The top rules (sorted by lift) revealed strong product relationships.  \n",
    "One example rule:\n",
    "\n",
    "**If 'bread' and 'butter' are purchased together, 'jam' is also purchased**  \n",
    "- Support: 0.25\n",
    "- Confidence: 0.75\n",
    "- Lift: 3.5\n",
    "\n",
    "### Implications\n",
    "This rule shows a complementary buying pattern.  \n",
    "A retailer could place these items near each other or bundle them in promotions to increase sales.  \n",
    "Such insights help in **cross-selling** and **marketing strategy** planning.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
